\chapter{Conclusion and Future Work}\label{sec:conclusion}
In this work, we incorporate workload properties into the construction of index structures. We look at two different workload properties and investigate how they can be used for index construction. Building on the foundation laid in previous work, we aim to achieve an automatic partitioning of data based on a representative workload sample. For this purpose, we design two specialized partitioning algorithms that analyze the workload sample and partition the data accordingly. We selected the frequency with which a key is requested and the query type of a key as promising properties for our algorithms. For the former property, the goal is to create partitions containing keys that are requested a similar amount of times, while for the latter property, we aim to create partitions that contain keys requested by mostly the same type of query. 

We first establish an understanding of the hyperparameters that our algorithms use. By investigating edge cases, we find that window size $w$ is essential to finding suitable partitions. If chosen too large, we might skip regions that should be their own partition according to our properties. On the other hand, if it is too small, we are susceptible to minor changes in our data, resulting in small partitions that are either caused by outliers or noise in the data. Moreover, we notice that the choice of $w$ and $\delta$ are essential to a good result, but we do not find a general or automatic recommendation for them.

Next, we try to reproduce the results of handcrafted partitions. When bulkloading an index with the partitions our purity algorithm produces, we find that we can outperform state-of-the-art index structures like ART and PGM. For the real-world dataset \verb|osm|, we can reduce the average lookup time from more than 350 ns to less than 250 ns. Even though this is a very specific scenario, we recognize the potential of our approach. To refocus our perspective, we look at worst-case scenarios for our algorithms, namely those that result either in one single, huge partition or those that produce a plethora of tiny partitions. The former is either caused by an insignificant data and workload distribution or a too large window size $w$, whereas the latter occurs when the window size $w$ is chosen so small that noise and outliers influence the algorithm. 

For a more realistic point of view, we investigate skewed workload distributions and find that our frequency algorithm, with a little fine-tuning, can create a reasonable amount of partitions, however because of the lack of optimization in our index based on frequency, we achieve lookup times that cannot compete with ART or PGM. For example, on the \verb|osm| dataset with a normal distribution as workload, we find lookup times of ART and PGM around 200 and 250 ns, while our index requires over 350 ns. However, it is still competitive with the TLX B-tree, which requires around 450 ns. We also find here that the underlying data distribution has less impact than ART and PGM, which both highly benefit from uniform, dense data. Furthermore, we investigate the influence of index misses on the lookup performance, where we only find that all indexes tend to perform on average faster when there are more misses and that our index is not an outlier and follows the same trend.

Based on all this, we recognize that the index construction using workload properties can be beneficial if there is an optimization in the index that processes the information. Otherwise, we can only achieve mediocre results with the drawback of having to run a partitioning algorithm before even building the index.

\paragraph{Future Work}
As previously mentioned, we do not have an automatic way to determine the parameters $w$ and $\delta$ for each combination of dataset and workload. Furthermore, tuning these manually can result in subpar partitions and a lot of manual experimentation. Essentially, apart from visual inspection, we do not know which configuration is good. A possible further work would therefore be to either select these parameters automatically or to develop a metric that measures the quality of a partitioning. This way, one would at least have an objective measure instead of only a visual cue. 

Another essential field for further research is the transformation of our index based on the information we obtain from the partitioning algorithms. The first part of this would be to implement the idea described in \Cref{sec:leaftransform}. Other possibilities include adding compression to some nodes based on the frequency information that we obtain. In general, the use of different data structures inside the index and their use still remains a wide-open topic. 

Lastly, one could look into identifying more workload properties or even combining them. We used the frequency and purity of partitions as metrics, but there are surely more properties that could be used.