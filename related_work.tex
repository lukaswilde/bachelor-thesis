\thispagestyle{plain}
\chapter{Related Work}

This chapter covers work related to Workload-based Data Partitioning. I also introduce the index structures used as baselines in the evaluation.

The well-known B$^+$-tree \cite{Bayer1970-rh} is the first basic index structure used in the comparison.

The second index used for comparison is the Adaptive Radix Tree (ART) \cite{Leis2013}. The authors recognize that the B$^+$-tree is widely used for disk-based database systems but indicate that it is unsuitable for modern-day main-memory databases. This is mainly due to the disparity of CPU cache sizes and main memory speed making main memory access times no longer uniform. Also, there is the problem of CPU stalls, which is caused by the CPU being unable to predict the result of comparisons easily. As comparisons are necessary to  traverse a B$^+$-tree, this causes more latency for the index. To overcome these problems, the authors introduce an improvement to Radix Trees, which uses certain parts of the keys directly to guide the search in the tree. While Radix Trees get rid of the afore-mentioned CPU stalls, they often have to make a global trade-off between tree height and space efficiency. This problem is solved by introducing adaptive nodes with varying capacities to hold child pointers. Results show that ART can outperform other main-memory data structures, with the only competitor being a hash table. As these store keys in a random order, they cannot support range queries efficiently and are only useful in specific scenarios.

To cover the last index structure used in the comparison, we need to look at another class of index structures that only emerged recently. Learned index structures generally try to leverage recent progress in the field of Machine Learning to improve index performance.

The Recursive Model Index (RMI) \cite{Kraska2018} introduces the concept that indexes are models that simply map keys to positions in a sorted array. The authors state that most modern index structures do not consider the data distribution and miss out on highly relevant optimizations. While most datasets don't follow simple patterns, they argue that Machine Learning approaches can be used to incorporate these patterns. One can look at traversing a B$^+$-tree as slowly reducing the error it takes to find the final location of the key (e.g.~ 100M possible records are reduced to 1M). The same approach can be adapted to Machine Learning models. While it is hard to guarantee that a single model will reduce the error from 100M (possible keys) to hundreds for the final search, it is reasonable for a model to do so from 100M to 10k. With this in mind, the authors construct a hierarchy of Machine Learning models, where each model picks the next layer's model that should be used to predict the position of the key. This hierarchy does not need to follow a strict tree structure; each model can cover an individual number of keys. The benefit of this architecture lies in the ability to customize the models. For example, the bottom layer of nodes could only represent linear regression models (as there are many leaves and linear regression models are quite inexpensive), while higher up, one could use more complex neural network structures. The data segmentation happens through the given structure and training of the internal node's models. While this paper introduces the use of the underlying data distribution for index construction, it suffers from the explainability of neural networks. Therefore, there are no clear properties that could be used for my work.

FITing-Tree \cite{Galakatos2019} tries to combine the ﬂexibility of traditional index structures with learning by indexing linear data segments. The data partitioning is done by a single pass over the sorted data. The segmentation algorithm aims to determine the data segments' bounds so that the relation between keys and positions in the sorted array can be approximated by a linear function. To give reliable performance estimates, an error parameter is used to indicate how much an estimated position is allowed to deviate from the real position. A new segment is created once a point falls outside an error cone that ensures this maximum deviation. Otherwise, the cone is adjusted by tightening its bounds. Once the segments are determined, they are indexed by a B$^+$-tree to find a key's corresponding segment, and a binary search is performed inside the segment to find the actual position of the key.

The authors of the Piecewise Geometric Model index (PGM) \cite{Ferragina:2020pgm}, which I used as the third baseline in the evaluation, tried to improve upon the ideas of FITing-Tree. While FITing-Tree's approach seemed reasonable, a disadvantage was the data segmentation. The authors note that the single-pass segmentation algorithm does not produce the optimal number of data segments, leading to more data segments, a larger tree height, and increased lookup time. By reducing the segmentation to the problem of constructing a convex hull and allowing the index to be built recursively, they could increase the lookup time and ensure provably efficient time and space bounds in the worst case.

TODO: Distribution-aware PGM

While learned index structures perform so well because they can adapt to the underlying data distribution, apart from the distribution-aware PGM, they do not consider the workload that will be executed. RMI partition the data indirectly through their models, FITing-Tree and PGM explicitly use segmentation algorithms before building the index to determine the data that belongs in one segment. However, workload information might be beneficial to index construction, e.g.~by indicating that certain data segments are not frequently requested. My work covers this problem: can workload information be used to improve data segmentation and thereby yield better performance?

Adaptive Hybrid Indexes \cite{Anneser2022} tackle the problem of selecting suitable encodings inside index structures to trade-off between space utilization and index performance. Compact indexes reduce the index's memory, allowing the database system to utilize that free memory to accelerate queries. However, they are naturally inferior to performance-optimized indexes. The decision of what encoding should be used on which part of the index is hard to make at build-time. Therefore, the authors propose to make these decisions at run-time. They introduce a framework that allows for monitoring the accesses across the index nodes when queries are processed, and based on these metrics, they classify whether nodes are cold or hot. Using so-called context-sensitive heuristic functions (CSHF), the framework determines, based on the classification of hot- and coldness, the memory budget, the historical classifications, and other properties, whether a node should have a compressed or performance-optimized encoding. Especially relevant to my work is the classification as hot or cold. Once the data is split into segments and inserted into a tree-like structure, it could be beneficial to modify the index based on this classification. While the authors do this at run-time, my work focuses on analyzing the workload before building the index. They use encodings for this, but one could also consider shifting leaves in the tree higher up to optimize for cache benefits.

    \begin{itemize}
    \item GENE \cite{Dittrich2021} for the approach to look at indexes as logical components and combining them, generic
    search briefly to iterate over starting options and give our partitioning as a possible better starting point.
    \end{itemize}

Distributed database systems are another field where the workload is used for partitioning. An example there is Schism \cite{Curino2010}. The motivation behind this approach is to improve the performance and scalability of distributed databases. Each tuple is represented as a node in a graph. Two nodes are connected if the corresponding tuples occur in the same transaction. The edges are weighted with the total amount of co-occurrences in transactions. Given a number of partitions $k$, the algorithm will ﬁnd a set of cuts of the edges that produces $k$ distinct partitions with roughly equal weight and minimal costs along the cut edges. The intuition behind this is that tuples that are often accessed in the same transaction should also reside in the same partition/node to optimize query processing. By minimizing the cost along the cut edges, pairs of tuples that are seldom accessed together are split into diﬀerent partitions, whereas often connected tuples stay in the same partition. While I don't look at transaction-based workloads in this work, there is a similarity in looking at workload properties to partition the data. Schism uses the frequency of co-occurrences to do this partitioning, which indicates that the frequency of query accesses could be a promising property to look at.